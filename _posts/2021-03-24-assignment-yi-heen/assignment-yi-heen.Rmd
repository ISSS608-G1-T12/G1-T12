---
title: "Investing 101: A visual and predictive guide for the rookie investor - Yi Heen"
description: |
  Existing financial data websites such as Yahoo Finance do a good job in providing historical price data and technical indicators, but the beginner investor lacks the knowledge to properly utilise and benefit from these. In addition, we have also identified several gaps in such websites.
  
  For one, these websites do not provide tools to allow the user to compare stocks meaningfully or zoom in to the statistical properties of financial returns. For example, a user is unable to conduct correlation analysis or visualize the distribution of returns. Secondly, these websites also do not provide any form of forecasting to aid in investorsâ€™ decisions.
    
  This project aims to improve on the current offering of financial data websites through, Exploratory Data Analysis, Time-Series Forecasting and also Time-Series Clustering.
  
author:
  - name: Yi Heen, Boey
    url: https://www.linkedin.com/in/yiheen-boey/
date: 03-24-2021
output:
  distill::distill_article:
    self_contained: false
    toc: true
    toc_depth: 3
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## 1. Overview

### 1.1 Background

Traditional financial data websites usually shy away from giving recommendations or advising users on investing decisions. While in depth studies are often conducted and used in academia or research, the outputs of these studies are often not accessible nor timely enough for the regular user. This means that these websites and their applications are limited in their use-cases. This can be due to issues arising from financial or legal liability if users act on the recommendations coming from these financial websites. As such we feel that this a gap between what is available publicly and user requirements. 

### 1.2 Objective and Motivation

This motivation of building this submodule and its functionality is **to bridge the gap between user requirements and current market offerings** and its main objective of this sub-section is to allow users to have a forecast of **how the stock prices are likely to move in order to identify potential entry/exit points.** 


### 1.3 Time Series Analysis and Forecasting

Time series data is a sequential collection of numerical data points. Particularly for investing, a time series data-set tracks the movement of a chosen index over a period of time with readings recorded at ***regular intervals*** (e.g. daily, monthly, yearly). 

Time series analysis allows users to uncover meaningful information and trends in data collected over time. Time series forecasting on the other hand, uses information contained within historical values and associated patterns to forecast future movements. Most often, we look at trends, cyclical fluctuations and issues of seasonality. 

### 1.4 Scope 

As a demonstration of the time series analysis and the forecasting capabilities of the proposed Shiny Application: The company ***APPLE***  will be used as an example for this report. 

For the purposes of this assignment, we will use the time series data from one single company - *Apple (Ticker: AAPL)*, as an example to illustrate the capabilities and functions of the application. There is no methodical reason to explain why Apple is used for the example other than the fact that it is one of the most recognisable companies in the market. The final product will be scaled up considerably to include more stocks that the users can choose to perform time series analysis and forecasts on. 


## 2. Proposed Visualization

### 2.1 Sketch of Proposed Visualization

### 2.2 Suggested Interactivity in the Proposed Visualization

## 3. Building the Visualization

### 3.1 Setting up the enviroment/libraries

First, we run this first line of code to clear the environment and remove existing R objects (if any)

```{r, echo=TRUE, eval=TRUE, fig.cap="Starting on a clean slate"}

rm(list=ls())

```

This code chunk checks if required packages are installed. If they are not installed, the next line of code will install them. The following line is then use to import the library into the current working environment.

```{r, echo=TRUE, eval=TRUE, fig.cap="Installing and importing libraries"}

packages = c('sf','tmap','tidyverse','forecast',
             'tseries','readxl','tidyquant',
             'dygraphs','TSstudio','plotly',
             'tsibble','ggplot2','tidymodels',
             'modeltime','modeltime.ensemble',
             'timetk','glmnet','randomForest')

for (p in packages) {
  if(!require(p,character.only = T)){
    install.packages(p)
  }
  library(p,character.only = T)
}

```

### 3.2 Data Preparation

*The application will use open source and live data obtained through the [**tidyquant**](https://cran.r-project.org/web/packages/tidyquant/index.html) package.* 

In the code chunk below, we define the key parameters for the time series data-set *ticker_data_daily*. Once again - in the final application these parameters are user defined/selected. 

1. The beginning date of the time series data *from_date*

2. The ending date of the time series data *to_date*

3. The ticker of the stock that we selected *AAPL*

```{r, echo=TRUE, eval=TRUE, out.width="100%", fig.cap="Importing the required data"}

from_date <- "2018-01-01"
to_date <- "2020-12-31"


ticker_selected <- "AAPL" 

ticker_data_daily <- tq_get(ticker_selected,
                            get = "stock.prices",
                            from = from_date,
                            to = to_date)

```

Next we call the glimpse function to take a quick look at the table, data structures, data types and data format to ensure everything is imported as it should be. Particularly, the date column in a time series data-set must be corrected identified as date in order to move on with analysis and forecasting. Since the data has been imported correctly as seen below, there is no need for any further transformations and we can proceed to the next step. 

```{r, eval=TRUE, out.width="100%"}

glimpse(ticker_data_daily)

```

We also drop the unneeded columns as we will be looking specifically at the adjusted close prices for this exercise.

Why do we use adjusted close prices?:
*The adjusted closing price amends a stock's closing price to accurately reflect that stock's value after taking corporate actions (e.g. stock splits, dividends) into account. These corporate actions can affect the closing prices. It is often used when examining historical returns or doing a detailed analysis of past performance.*

```{r, echo=TRUE, eval=TRUE}

ticker_daily_adjclose <- subset(ticker_data_daily, select = -c(3,4,5,6,7))

```

### 3.3 Data Visualization - Time Series

We can now plot a timeseries chart for a visual analysis of the data. The chart below shows the trend of stock prices for the years 2018-2020. The slider below it also allows users to zoom in on a data range to have a closer look at the data on a day to day basis. In the Shiny application where users can select the date-range of the data to import, the slider will come in handy when selected date ranges are big. 

```{r,layout='l-page', echo=TRUE, eval=TRUE, out.width="100%", fig.cap="Time Series Plot for AAPL Stock Adjusted Closing Prices."}

ticker_daily_adjclose %>%
    plot_time_series(date, adjusted, 
                     .smooth_size = 0.2,
                     .title = "AAPL Adjusted Closing Prices 2018-2020",
                     .interactive = TRUE,
                     .plotly_slider = TRUE)


```
While not part of this exercise, we can also quickly look at how to compare our time series in relation to the one (AAPL) that we are looking at. This code chunk used once again relies on the *tidyquant* package to pull in data from the remain FAANG stocks and also TSLA which has been performing phenomenally over the past year.

```{r, echo=TRUE, eval=TRUE, out.width="100%"}

from_date <- "2018-01-01"
to_date <- "2020-12-31"


ticker_selected <- c("AAPL","AMZN","FB","GOOG","NFLX","TSLA") 

ticker_data_daily <- tq_get(ticker_selected,
                            get = "stock.prices",
                            from = from_date,
                            to = to_date)

ticker_daily_adjclose <- subset(ticker_data_daily, select = -c(3,4,5,6,7))

```

Once we have the data, now we will plot them as facet charts so that we can have a closer look at the trends each time series has (code chunk above it). In this plot, the scales are fixed, i.e. the y-axes are synchronized across the rows. This is not very useful to us as the high prices of the AMZN, NFLX, GOOG, TSLA stocks make the trends in AAPL and FB imperceptible. 

```{r,fig.align="center",layout='l-page',echo=TRUE, eval=TRUE, out.width="100%", fig.cap="Plotting other tickers for comparison"}

ticker_daily_adjclose %>%
    group_by(symbol) %>%
    plot_time_series(date, adjusted, 
                     .smooth_size = 0.2,
                     .facet_ncol  = 2,    
                     .facet_scales = "fixed",
                     .interactive = TRUE)


```

Here we have added another line '.facet_scales = "free"', this frees up the scale to be plotted independently of other charts. Immediately, the trends in AAPL and FB are showing and the chart becomes a lot more useful. More comparisons across different time series can be found in the other sub-modules. As such, we will now go into forecasting proper. 

```{r,fig.align="center",layout='l-page',echo=TRUE, eval=TRUE, out.width="100%", fig.cap="Freeing the Facet Scales"}

ticker_daily_adjclose %>%
    group_by(symbol) %>%
    plot_time_series(date, adjusted, 
                     .smooth_size = 0.2,
                     .facet_ncol  = 2, 
                     .facet_scales = "free", #added
                     .interactive = TRUE)


```

### 3.4 Time Series Forecasting


#### Step 1 - Data prep

```{r, eval=TRUE, out.width="100%"}

# Resetting tickers to just AAPL

from_date <- "2018-01-01"
to_date <- "2020-12-31"


ticker_selected <- "AAPL" 

ticker_data_daily <- tq_get(ticker_selected,
                            get = "stock.prices",
                            from = from_date,
                            to = to_date)

ticker_daily_adjclose <- subset(ticker_data_daily, select = -c(3,4,5,6,7))

```

Before we go into time series forecasting proper. We will need to further breakdown the data into a training dataset to build the model and a testing dataset which will be used to determine the efficacy of the model. For the purposes of this paper, we will use 3 months worth of data for the testing dataset but this can be changed. This can be changed by the user but this in turn will affect the model outputs in terms of accuracy and amount of data available for training the models.

The following code chunk is then used to create the training and testing sets. 


```{r,fig.align="center",layout='l-page',echo=TRUE, eval=TRUE, out.width="100%", fig.cap="Splitting the time series dataset"}

splits <- time_series_split(ticker_daily_adjclose, assess = "3 months", cumulative = TRUE)

splits %>%
    tk_time_series_cv_plan() %>%
    plot_time_series_cv_plan(date, adjusted,
                             .title = "Time Series Cross Validation Plan",
                             .interactive = TRUE)

```

#### Step 2 - Modeling

Next we create a Feature Engineering Recipe that can be applied to the data in order to create features that machine learning models can utilize. 

```{r,echo=TRUE, eval=TRUE, layout='l-page'}

recipe_spec <- recipe(adjusted ~ date, training(splits)) %>%
    step_timeseries_signature(date) %>%
    step_rm(matches("(.iso$)|(.xts$)")) %>%
    step_normalize(matches("(index.num$)|(_year$)")) %>%
    step_dummy(all_nominal()) %>%
    step_fourier(date, K = 1, period = 12)

recipe_spec %>% prep() %>% juice()

```
We will then create a few forecasting models that the application will use to forecast the forward prices. More literature can be found in the hyperlinks below and the code chunk follows below.

The models that we have included so far are:

1. [ARIMA](https://rdrr.io/cran/forecast/man/Arima.html)

2. [Prophet](https://cran.r-project.org/web/packages/prophet/prophet.pdf)

3. [Elastic Net](https://cran.r-project.org/web/packages/glmnet/index.html)

4. [Decision Tree](https://rstudio-pubs-static.s3.amazonaws.com/439628_922355dd3b0e442db12c7a408769b63e.html)

5. [Prophet with XG Boost](https://rdrr.io/cran/modeltime/man/prophet_boost.html)

```{r,echo=TRUE, eval=TRUE}

# 1 ARIMA

model_spec_arima <- arima_reg() %>%
    set_engine("auto_arima")

wflw_fit_arima <- workflow() %>%
    add_model(model_spec_arima) %>%
    add_recipe(recipe_spec %>% step_rm(all_predictors(), -date)) %>%
    fit(training(splits))

# 2 Prophet

model_spec_prophet <- prophet_reg() %>%
    set_engine("prophet")

wflw_fit_prophet <- workflow() %>%
    add_model(model_spec_prophet) %>%
    add_recipe(recipe_spec %>% step_rm(all_predictors(), -date)) %>%
    fit(training(splits))

# 3 Elastic Net

model_spec_glmnet <- linear_reg(
    mixture = 0.9,
    penalty = 4.36e-6
) %>%
    set_engine("glmnet")

wflw_fit_glmnet <- workflow() %>%
    add_model(model_spec_glmnet) %>%
    add_recipe(recipe_spec %>% step_rm(date)) %>%
    fit(training(splits))

# 4 Random Forest

model_spec_rf <- rand_forest(trees = 500, min_n = 50) %>%
  set_engine("randomForest")

wflw_fit_rf <- workflow() %>%
  add_model(model_spec_rf) %>%
  add_recipe(recipe_spec %>% step_rm(date)) %>%
  fit(training(splits))

# 5 Boosted Prophet

model_spec_prophet_boost <- prophet_boost() %>%
  set_engine("prophet_xgboost", daily.seasonality = TRUE) 

wflw_fit_prophet_boost <- workflow() %>%
  add_model(model_spec_prophet_boost) %>%
  add_recipe(recipe_spec) %>%
  fit(training(splits))

```
Next we then create a modeltime_table using the modeltime package. This is to allow for easier calibration and refitting at the later parts of the forecasting.

```{r,echo=TRUE, eval=TRUE}

ticker_models <- modeltime_table(
    wflw_fit_arima,
    wflw_fit_prophet,
    wflw_fit_glmnet,
    wflw_fit_rf,
    wflw_fit_prophet_boost
    
)

ticker_models

```

##### Step 3 - Calibration and Testing


Next we calibrate the model using the test data that we obtain by splitting the full time series dataset into training and testing sets. Using the codechunk below, we also plot the forecasts of all 5 models against that of the actual test data. This will give us a rough idea of the accuracy of model.

```{r,echo=TRUE, eval=TRUE, fig.align="center",layout='l-page'}

calibration_table <- ticker_models %>%
  modeltime_calibrate(testing(splits))

calibration_table %>%
  modeltime_forecast(actual_data = ticker_daily_adjclose) %>%
  plot_modeltime_forecast(.interactive = TRUE,
                          .plotly_slider =  TRUE)


```

Of course, it is obvious that not all models workout. From the previous chart, we can tell that the Random Forest model and the GLMNet Model are producing results way off from the test data set. This means that we should consider dropping those models while building the final application. We then build a modeltime_accuracy table to determine which models to drop quantitatively. Using MAPE as a measure, we will select only the top 3 models in building the ensemble model, which means that in this scenario, we will drop the Random Forest and GLMNet Models.

```{r, fig.align="center",layout='l-page'}

calibration_table %>%
  modeltime_accuracy() %>%
  table_modeltime_accuracy(.interactive = TRUE)

```

##### Step 4 - Building the Ensemble model

When we present the timeseries forecastting results, we will only present a unified/aggregated set of forecasts from the ensemble model to avoid confusing the user. In our application, the ensemble model uses the mean of the forecast of the models with the top 3 MAPE measures. The mean of the forecasts will then be shown in the chart as the expected future share price of the stock that the users will use for decision making. The ensemble model approach is used to combine the strengths and eliminate the weaknenss of each model.

```{r}

ensemble_fit <- ticker_models %>%
    # Remove Random Forest and GLMNET due to low accuracy - CHANGE TO INTERACTIVE
    filter(.model_id != 3 & .model_id != 4 ) %>%
    ensemble_average(type = "mean")

ensemble_fit

```

Next we plot the ensemble model results against the test data. From the plot, we can see that the trend of the forecast is largely similar to that of the test data.

```{r, fig.align="center",layout='l-page'}

calibration_tbl <- modeltime_table(ensemble_fit) %>%
    modeltime_calibrate(testing(splits))

# Forecast vs Test Set
calibration_tbl %>%
    modeltime_forecast(
        new_data    = testing(splits),
        actual_data = ticker_daily_adjclose
    ) %>%
    plot_modeltime_forecast(.interactive = TRUE)

```

Finally, we refit the ensemble model results against the full dataset in order to forecast forward, this is done using the model_refit function. We can also see the confidence intervals of the future prices. The larger the confidence interval is will indicate a more volatile stock price. This is also the final visual that users will interact with when using the shiny application. The steps taken to get to this point are largely transparent to the user and will not be shown. 

```{r, fig.align="center",layout='l-page'}

refit_tbl <- calibration_tbl %>%
    modeltime_refit(ticker_daily_adjclose)

refit_tbl %>%
    modeltime_forecast(
        h = "3 months",
        actual_data = ticker_daily_adjclose
    ) %>%
    plot_modeltime_forecast(.interactive = TRUE,
                            .plotly_slider = TRUE)

```

## 4. Conclusions and Insights


## 5. References

https://blogs.oracle.com/ai-and-datascience/post/performing-a-time-series-analysis-on-the-sampp-500-stock-index#:~:text=Time%2Dseries%20analysis%20is%20a,to%20predict%20future%20stock%20values 

https://machinelearningmastery.com/time-series-forecasting-performance-measures-with-python/


https://yiqiaoyin.files.wordpress.com/2017/05/time-series-analysis-on-stock-returns.pdf

https://cran.r-project.org/web/packages/modeltime.ensemble/vignettes/getting-started-with-modeltime-ensemble.html

